// WGAN-GP

python train.py --dataset celeba --dataroot /path/to/traindata/ --testroot /path/to/testdata/ --cuda --nz 128 \
    --sigmasqz 1.0 --lr_eg 0.0001 --lr_di 0.0001 --beta1 0.5 --beta2 0.9 --niter 165 --check_every 100 \
    --workers 6 --outf /path/to/results/ --batchSize 64 --test_every 100 --addsamples 10000 --manualSeed 321 \
    --wganloss

// WAE

python train.py --dataset celeba --dataroot ./data --testroot /path/to/testdata/ --cuda --nz 128 \
    --sigmasqz 1.0 --lr_eg 0.001 --niter 55 --decay_steps 30 50 --decay_gamma 0.4 --check_every 100 \
    --workers 8 --recloss --mmd --bnz --outf /path/to/results/ --lbd 100 --batchSize 256 --detenc --useenc \
    --test_every 20 --addsamples 10000 --manualSeed 321

// Wasserstein++

python train.py --dataset mnist --dataroot ./data --cuda --nz 8 \
    --sigmasqz 1.0 --lr_eg 0.0003 --niter 165 --decay_steps 100 140 --decay_gamma 0.4 --check_every 100 \
    --workers 4 --recloss --mmd --bnz --outf ./outf/wpp --lbd 100 --batchSize 32 --detenc --useenc \
    --test_every 20 --addsamples 10000 --manualSeed 321 --wganloss --useencdist --lbd_di 0.000025 --intencprior

To learn the rate-constrained encoder and the stochastic mapping run the 
following (parameters again for the experiment on the CelebA data set):

python train.py --dataset celeba --dataroot /path/to/traindata/ --testroot /path/to/testdata/ --cuda --nz 128 \
    --sigmasqz 1.0 --lr_eg 0.001 --niter 55 --decay_steps 30 50 --decay_gamma 0.4 --check_every 100 \
    --workers 6 --recloss --mmd --bnz --batchSize 256 --useenc --comp --freezedec --test_every 100 \
    --addsamples 10000 --manualSeed 321 --outf /path/to/results/ --netG /path/to/trained/generator \
    --nresenc 2 --lbd 300 --ncenc 8

